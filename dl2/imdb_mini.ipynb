{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Language Model\n",
    "\n",
    "Key papers:\n",
    "\n",
    "'Learned in Translation: Contextualized Word Vectors' McCann et al. 2017\n",
    "\n",
    "'Regularizing and Optimizing LSTM Language Models', Merity et al. (2017)\n",
    "\n",
    "'A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay', Smith (2018)\n",
    "\n",
    "Here we are testing on subset of imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import html\n",
    "import sys\n",
    "from subprocess import call\n",
    "\n",
    "import torch\n",
    "\n",
    "from fastai.text import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active CUDA Device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "print('Active CUDA Device: GPU', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'    #beginning of sentence tag, useful for model to know this\n",
    "FLD = 'xfld'    #data field tag\n",
    "\n",
    "PATH=Path('..')/'data/imdb/aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'tmp', 'train', 'models', 'imdb.vocab', 'imdbEr.txt', 'README']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier\n",
    "CLAS_PATH=Path('..')/'data/imdb/imdb_clas'\n",
    "CLAS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "#Language Model\n",
    "LM_PATH=Path('..')/'data/imdb/imdb_lm'\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsup for unlabelled\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "\n",
    "def get_texts(path):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(CLASSES):\n",
    "        #The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell\n",
    "        for fname in (path/label).glob('*.*'):\n",
    "            #eg ../data/imdb/aclImdb/train/neg/1696_1.txt\n",
    "            texts.append(fname.open('r').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "trn_texts, trn_labels = get_texts(PATH/'train')\n",
    "val_texts, val_labels = get_texts(PATH/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(trn_texts): 75000, len(val_texts): 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'len(trn_texts): {len(trn_texts)}, len(val_texts): {len(val_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip data - identifying clipped data with '_mini' postfix\n",
    "trn_texts = trn_texts[:1500]\n",
    "trn_labels = trn_labels[:1500]\n",
    "val_texts = val_texts[:500]\n",
    "val_labels = val_labels[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['labels', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make randomness reproducible\n",
    "np.random.seed(42)\n",
    "#randomly shuffle this list\n",
    "trn_idx = np.random.permutation(len(trn_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our randomly sorted training and validation lists-generally a good idea to do this\n",
    "trn_texts = trn_texts[trn_idx]\n",
    "val_texts = val_texts[val_idx]\n",
    "\n",
    "trn_labels = trn_labels[trn_idx]\n",
    "val_labels = val_labels[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':val_labels}, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for training data, remove unsupervised\n",
    "df_trn[df_trn['labels']!=2].to_csv(CLAS_PATH/'train_mini.csv',header=False, index=False)\n",
    "df_val.to_csv(CLAS_PATH/'test_mini.csv', header=False, index=False)\n",
    "\n",
    "#write the classes to a file ie neg pos unsup\n",
    "(CLAS_PATH/'classes_mini.txt').open('w').writelines(f'{o}/n' for o in CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use more data for training than the given split\n",
    "trn_texts,val_texts = sklearn.model_selection.train_test_split(np.concatenate([trn_texts, val_texts]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise classifications to zero\n",
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':[0]*len(trn_texts)}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':[0]*len(val_texts)}, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn.to_csv(LM_PATH/'train_mini.csv', header=False, index=False)\n",
    "df_val.to_csv(LM_PATH/'test_mini.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Tokens\n",
    "\n",
    "Turn text into a a list of tokens using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this makes pandas more efficient-when passed in to pandas, returns an iterator to iterate through chunks, then loop through these chinks of the dataframe\n",
    "CHUNKSIZE = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile a regular expression pattern, returning a pattern object\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "#this may not catch all badly formatted text, may need to add to/modify for other input datasets\n",
    "def fixup(text_str):\n",
    "    text_str = text_str.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(text_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max thread count: 8\n"
     ]
    }
   ],
   "source": [
    "print(f'max thread count: {len(os.sched_getaffinity(0))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df, n_lbls=1):\n",
    "    #.iloc[<row_selection>,<col_selction>] here default is column 0 only\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 '+ df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls+1, len(df.columns)):\n",
    "        texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = texts.apply(fixup).values.astype(str)\n",
    "    #type(texts): <class 'pandas.core.series.Series'>\n",
    "    #uses ProcessPoolExcutor with 1/2 of the cpu's, pass in a series to tokenize\n",
    "    start = timer()\n",
    "    #significant speed up gained through multi processing\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    end = timer()\n",
    "    print(f'elapsed: {end - start}')\n",
    "    return tok, list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(tf_reader, n_lbls):\n",
    "    #iterate over the TextFileReader object in chunks\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(tf_reader):\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r, n_lbls)\n",
    "        tok += tok_\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(df_trn): <class 'pandas.io.parsers.TextFileReader'>\n"
     ]
    }
   ],
   "source": [
    "df_trn = pd.read_csv(LM_PATH/'train_mini.csv', header=None, chunksize=CHUNKSIZE)\n",
    "df_val = pd.read_csv(LM_PATH/'test_mini.csv', header=None, chunksize=CHUNKSIZE)\n",
    "#note is not a dataframe\n",
    "print(f'type(df_trn): {type(df_trn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "elapsed: 4.02717979100089\n",
      "0\n",
      "elapsed: 2.68446169300114\n"
     ]
    }
   ],
   "source": [
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)\n",
    "#note smaller output than full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LM_PATH/'tmp').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n-xbos-xfld-1-in-1914-,-charlie-chaplin-began-making-pictures-.-these-were-made-for-mack-sennett-(-also-known-as-\"-keystone-studios-\"-)-and-were-literally-churned-out-in-very-rapid-succession-.-the-short-comedies-had-very-little-structure-and-were-completely-ad-libbed-.-as-a-result-,-the-films-,-though-popular-in-their-day-,-were-just-awful-by-today-\\'s-standards-.-many-of-them-bear-a-strong-similarity-to-home-movies-featuring-obnoxious-relatives-mugging-for-the-camera-.-many-others-show-the-characters-wander-in-front-of-the-camera-and-do-pretty-much-nothing-.-and-,-regardless-of-the-outcome-,-keystone-sent-them-straight-to-theaters-.-my-assumption-is-that-all-movies-at-this-time-must-have-been-pretty-bad-,-as-the-keystone-films-with-chaplin-were-very-successful-.-\\n\\n-the-charlie-chaplin-we-know-and-love-today-only-began-to-evolve-later-in-chaplin-\\'s-career-with-keystone-.-by-1915-,-he-signed-a-new-lucrative-contract-with-essenay-studios-and-the-films-improved-dramatically-with-chaplin-as-director-.-however-,-at-times-these-films-were-still-very-rough-and-not-especially-memorable-.-no-,-chaplin-as-the-cute-little-tramp-was-still-evolving-.-in-1916-,-when-he-switched-to-mutual-studios-,-his-films-once-again-improved-and-he-became-the-more-recognizable-nice-guy----in-many-of-the-previous-films-he-was-just-a-jerk-(-either-getting-drunk-a-lot-,-beating-up-women-,-provoking-fights-with-innocent-people-,-etc-.-)-.-the-final-evolution-of-his-little-tramp-to-classic-status-occurred-in-the-1920s-as-a-result-of-his-full---length-films-.-\\n\\n-it-\\'s-interesting-that-this-film-is-called-t_up-twenty-t_up-minutes-of-t_up-love-since-the-film-only-lasts-about-10-minutes-!-oh-well-.-the-plot-,-what-little-there-is-,-involves-the-little-tramp-in-the-park-.-a-couple-wants-to-neck-but-inexplicably-,-charlie-insists-on-practically-sitting-on-the-couple-\\'s-lap-and-really-annoying-them-.-i-ca-n\\'t-understand-why-and-the-short-consists-of-charlie-wandering-about-the-park-annoying-these-people-and-some-others-later-in-the-film-.-perhaps-he-was-looking-for-a-threesome-,-i-do-n\\'t-know-.-but-the-film-lacks-coherence-and-just-is-n\\'t-particularly-funny----even-when-people-start-slapping-each-other-and-pushing-each-other-in-the-lake-.-a-typical-poor-effort-before-chaplin-began-to-give-his-character-a-plot-and-personality-.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first 100 chars example of data - t_up - indicates token is uppercase\n",
    "'-'.join(tok_trn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'tok_trn_mini.npy', tok_trn)\n",
    "np.save(LM_PATH/'tmp'/'tok_val_mini.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(LM_PATH/'tmp'/'tok_trn_mini.npy')\n",
    "tok_val = np.load(LM_PATH/'tmp'/'tok_val_mini.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 22882),\n",
       " ('.', 19703),\n",
       " (',', 18406),\n",
       " ('a', 11194),\n",
       " ('and', 10682),\n",
       " ('of', 9651),\n",
       " ('to', 9402),\n",
       " ('is', 7350),\n",
       " ('it', 6693),\n",
       " ('i', 6427),\n",
       " ('in', 6136),\n",
       " ('this', 5984),\n",
       " ('that', 5317),\n",
       " ('\"', 4953),\n",
       " (\"'s\", 4233),\n",
       " ('was', 4005),\n",
       " ('-', 3846),\n",
       " ('\\n\\n', 3650),\n",
       " ('movie', 3594),\n",
       " ('for', 3041),\n",
       " ('but', 3034),\n",
       " ('with', 2934),\n",
       " ('as', 2741),\n",
       " (\"n't\", 2733),\n",
       " ('film', 2704)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit as over this code gets 'clunky'\n",
    "#also for classification using >60k doesnt help anyway\n",
    "MAX_VOCAB = 60000\n",
    "MIN_FREQ = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of words, create index for this list\n",
    "\n",
    "itos: index to string\n",
    "stoi: string to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index those tokens that appear more than 2x\n",
    "itos = [o for o,c in freq.most_common(MAX_VOCAB) if c>MIN_FREQ]\n",
    "itos.insert(0, '_pad_')\n",
    "#use if not in vocab\n",
    "itos.insert(0, '_unk_')\n",
    "itos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9240"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#default to 0 if not in dict\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index each token for each review. Call for every word, for every sentence\n",
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "trn_lm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'trn_ids_mini.npy', trn_lm)\n",
    "np.save(LM_PATH/'tmp'/'val_ids_mini.npy', val_lm)\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos_mini.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids_mini.npy')\n",
    "val_lm = np.load(LM_PATH/'tmp'/'val_ids_mini.npy')\n",
    "itos = pickle.load(open(LM_PATH/'tmp'/'itos_mini.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9240, 1800)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(itos)\n",
    "VOCAB_SIZE, len(trn_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wikitext103 Conversion\n",
    "\n",
    "42:00\n",
    "\n",
    "Instead of pretraining on Imagenet, for NLP we can pretrain on a large subset of Wikipedia\n",
    "\n",
    "If pre-train classifier by first creating a language model then fine tune that as a classifier-helpful in L4 2017\n",
    "\n",
    "IMDB not that different to english docs - train a good @ English LM then fine tune.\n",
    "\n",
    "S. Merity created WikiText-103 contains all articles extracted from Wikipedia (ignoring smaller atricles):\n",
    "\n",
    "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\n",
    "\n",
    "JH trained this Language Model - start with these weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -nH -r -np http://files.fast.ai/models/wt103/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our model needs to have exactly the same embedding size, number of hidden layers and number of layers as per Jeremy's wikitext103 LM\n",
    "EMBEDDING_SIZE = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = PATH/'models'/'wt103'\n",
    "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.load uses Python's unpickling facilities but treats storages, which underlie tensors, specially\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eg row_m: array([-0.0183 , -0.13826,  0.01438, -0.01285,  0.00407,  0.01944,  0.01149, -0.13282, -0.02295, ... ], dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to map our itos to itos for wikitext, which is easy as we have the itos for wikitext103\n",
    "itos_wiki = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
    "#-1 means not in wikitext dictionary\n",
    "stoi_wiki = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos_wiki)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty embedding matrix\n",
    "new_wgts = np.zeros((VOCAB_SIZE, EMBEDDING_SIZE), dtype=np.float32)\n",
    "#go through every work in imdb vocab\n",
    "for i, w in enumerate(itos):\n",
    "    #look it up in wiki vocab\n",
    "    r = stoi_wiki[w]\n",
    "    #use mean if our string doesnt exist in wiki (-1 means not in wikitext dict)\n",
    "    new_wgts[i] = enc_wgts[r] if r>=0 else row_m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T: convert to torch tensor and put on gpu. Replace our weights\n",
    "wgts['0.encoder.weight'] = T(new_wgts)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_wgts))\n",
    "#decoder (turns final prediction back into a word) uses same weights\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_wgts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "Word2Vec is a single embedding matrix - each word has a vector and thats it.\n",
    "\n",
    "A single layer (input layer) form a pre-trained linear model - pre-trained on a co-ocurrence matrix. No reason to beleive it has learnt anything about the English languguage nor do we expect it to have any great capabilities.\n",
    "\n",
    "This language model had a 400 dimensional embedding matrix, 3 hidden layers with 1150 activations per layer + reg - state of the art AWD LSTM.\n",
    "\n",
    "Single layer of linear model vs 3 layer RNN - very different capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of hidden activation per LSTM layer\n",
    "n_hid= 1150\n",
    "#number of LSTM layers to use in the architecture\n",
    "n_layers = 3\n",
    "wd=1e-7\n",
    "#grab 70 at a time\n",
    "bptt=70\n",
    "bs=52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "#for nlp better to use the defaults below\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(487718, 9379)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate all our docs together\n",
    "t = len(np.concatenate(trn_lm))\n",
    "t, t//bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For our Language Model we concatenate all docs, will be continually trying to predict: what is the next word\n",
    "#(The 3 lines below are the same code Jeremy also used to train the wikitext103 model from scratch)\n",
    "#We start by initializing the LanguageModelLoader with a big list of numbers (all docs concatenated together).\n",
    "#Then we batchify this - break total data into bs pieces ie here 64 pieces.\n",
    "#ie data = data.reshape(self.bs, -1).T \n",
    "#will thus have 64 columns and total_dala/64 rows\n",
    "        \n",
    "#batchify and get_batch very similar to awd-lstm-lm.utils functions\n",
    "        \n",
    "#*1:05 - key ideas here, review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when we iterate over LanguageModelLoader, the sequence length is changed with a normal distribution and significantly changed 5% of time\n",
    "#on first iteration: seq_len = bptt+5*5 ie 95 here\n",
    "#subseqently: seq_len = max(5, int(np.random.normal(bptt, 5))) where 5% of time the mean bptt=bptt/2\n",
    "#see awd-lstm-lm.finetune.train()\n",
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "\n",
    "#LanguageModelData returns a RNN_Learner with SequentialRNN model\n",
    "#(The SequentialRNN layer is the native torch's Sequential wrapper that puts the RNN_Encoder and LinearDecoder layers sequentially in the model.)\n",
    "model_data = LanguageModelData(PATH, pad_idx=1, nt=VOCAB_SIZE, trn_dl=trn_dl, val_dl=val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropouts - through experimentation these work well.\n",
    "#less data need more dropout - good ratios, just tune the multiplier (if overfitting increase the multiplier)\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a model data object we gan grab the model - which will give us a learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout: dropout to apply to the activations going from one LSTM layer to another\n",
    "#dropouti (float): dropout to apply to the input layer.\n",
    "#wdrop (float): dropout used for a LSTM's internal (or hidden) recurrent weights.\n",
    "#dropoute (float): dropout to apply to the embedding layer.\n",
    "#dropouth (float): dropout to apply to the activations going from one LSTM layer to another\n",
    "kwargs = {'dropouti': drops[0], 'dropout': drops[1], 'wdrop': drops[2], 'dropoute': drops[3], 'dropouth': drops[4]}\n",
    "\n",
    "#returns a RNN_Learner with model ~ SequentialRNN(RNN_Encoder(...), LinearDecoder(...))\n",
    "learner = model_data.get_model(opt_fn = opt_fn, emb_sz = EMBEDDING_SIZE, n_hid = n_hid, n_layers = n_layers, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "all the work happens in get_model(). - the key part - where we implement **AWD LSTM** and use the backbone+head\n",
    "\n",
    "Creates an encoder (RNN_Encoder) then creates a SequentialRNN and sticks on top of the encoder a LinearDecoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Learner.unfreeze of SequentialRNN(\n",
       "  (0): RNN_Encoder(\n",
       "    (encoder): Embedding(9240, 400, padding_idx=1)\n",
       "    (encoder_with_dropout): EmbeddingDropout(\n",
       "      (embed): Embedding(9240, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDrop(\n",
       "        (module): LSTM(400, 1150, dropout=0.105)\n",
       "      )\n",
       "      (1): WeightDrop(\n",
       "        (module): LSTM(1150, 1150, dropout=0.105)\n",
       "      )\n",
       "      (2): WeightDrop(\n",
       "        (module): LSTM(1150, 400, dropout=0.105)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout(\n",
       "    )\n",
       "    (dropouths): ModuleList(\n",
       "      (0): LockedDropout(\n",
       "      )\n",
       "      (1): LockedDropout(\n",
       "      )\n",
       "      (2): LockedDropout(\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=9240, bias=False)\n",
       "    (dropout): LockedDropout(\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.metrics = [accuracy]\n",
    "learner.unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we do a fit just on the last layer. The way this is setup the last layer is the embedding weights-which will be the thing that is most wrong.\n",
    "\n",
    "Train a single epoch of just the embedding weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "lrs=lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
<<<<<<< Updated upstream
=======
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
>>>>>>> Stashed changes
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      4.632719   4.311474   0.243408  \n",
      "\n",
      "elapsed: 24.66835709100087\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)\n",
    "end = timer()\n",
    "print(f'elapsed: {end - start}')\n",
    "#showing seq_len and bptt values:\n",
    "# 0%|          | 0/6870 [00:00<?, ?it/s]initial seq_len: 95, bptt: 70\n",
    "# 0%|          | 1/6870 [00:01<2:29:15,  1.30s/it, loss=11]seq_len: 36, bptt: 70\n",
    "# 0%|          | 2/6870 [00:01<1:20:31,  1.42it/s, loss=11]seq_len: 73, bptt: 70\n",
    "# 0%|          | 3/6870 [00:01<1:00:29,  1.89it/s, loss=11]seq_len: 73, bptt: 70\n",
    "# 0%|          | 4/6870 [00:01<50:36,  2.26it/s, loss=11]  seq_len: 65, bptt: 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_loss is a cross entropy loss (perplexity = e^val_loss), accuracy is how often do we get next word correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm_last_fit_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('lm_last_fit_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
<<<<<<< Updated upstream
=======
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
>>>>>>> Stashed changes
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      4.554038   4.347477   0.23547   \n",
      "\n",
      "elapsed: 22.70915235800021\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)\n",
    "end = timer()\n",
    "print(f'elapsed: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start doing a few epochs of the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
<<<<<<< Updated upstream
=======
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
>>>>>>> Stashed changes
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      4.455422   4.198092   0.252937  \n",
      "    1      4.270598   4.101986   0.261059  \n",
      "    2      4.055444   4.080953   0.264046  \n",
      "    3      3.889293   4.078577   0.264969  \n",
      "    4      3.745949   4.085079   0.262757  \n",
      "    5      3.651408   4.07044    0.265233  \n",
      "    6      3.493323   4.125102   0.26273   \n",
      "    7      3.394642   4.158749   0.259636  \n",
      "    8      3.252936   4.232037   0.256156  \n",
      "    9      3.170588   4.264293   0.254935  \n",
      "    10     3.117307   4.27552    0.255633  \n",
      "    11     3.023912   4.313365   0.253026  \n",
      "    12     3.043767   4.315423   0.25304   \n",
      "    13     2.944658   4.348093   0.252286  \n",
      "    14     2.956883   4.345921   0.251975  \n",
      "\n",
      "elapsed: 343.30368543700024\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)\n",
    "end = timer()\n",
    "print(f'elapsed: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without using wiki103 weights\n",
    "#epoch      trn_loss   val_loss   accuracy   \n",
    "#    0      6.364579   6.188742   0.046889  \n",
    "#    14     4.899093   4.919841   0.187322 \n",
    "#with\n",
    "#epoch      trn_loss   val_loss   accuracy   \n",
    "#    0      4.470828   4.169173   0.256953  \n",
    "#    14     2.976444   4.30655    0.2571  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.21750212099356"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(4.307)  #2 years ago perplixity state of art was >100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss compares with Lesson 4 - Academic worlds best in 2017 - aftewr 14 epochs val loss of 4.23. \n",
    "\n",
    "Here after 1 epoch on full model we have a 4.12 loss \n",
    "\n",
    "ie by pretraining on wikitext103 better loss after 1 epoch than best loss for Lesson 4 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for full dataset after 5 hrs (on GTX1080Ti):\n",
    "#14     4.045284   4.072163   0.299391  \n",
    "\n",
    "#1/50th dataset after 213 seconds (356 seconds on GTX1070):\n",
    "#14     4.899093   4.919841   0.187322  \n",
    "\n",
    "#1/100th dataset after 96 seconds:\n",
    "#14     5.937064   5.737786   0.097723  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm1_mini') #saves whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('lm1_enc_mini') #saves just the encoder part of the sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(CLAS_PATH/'train_mini.csv', header=None, chunksize=CHUNKSIZE)\n",
    "df_val = pd.read_csv(CLAS_PATH/'test_mini.csv', header=None, chunksize=CHUNKSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "elapsed: 3.8653473270005634\n"
     ]
    }
   ],
   "source": [
    "tok_trn, trn_labels = get_all(tf_reader=df_trn, n_lbls=1)\n",
    "tok_val, val_labels = get_all(tf_reader=df_trn, n_lbls=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CLAS_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'tok_trn_mini.npy', tok_trn)\n",
    "np.save(CLAS_PATH/'tmp'/'tok_val_mini.npy', tok_val)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'trn_labels_mini.npy', trn_labels)\n",
    "np.save(CLAS_PATH/'tmp'/'val_labels_mini.npy', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#about to reload same encoder, reuse our tokens from languiage model\n",
    "tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn_mini.npy')\n",
    "tok_val = np.load(CLAS_PATH/'tmp'/'tok_val_mini.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9240"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos = pickle.load((LM_PATH/'tmp'/'itos_mini.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_clas = np.array([[stoi[o] for o in p] for p in tok_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(CLAS_PATH/'tmp'/'trn_ids_mini.npy', trn_clas)\n",
    "np.save(CLAS_PATH/'tmp'/'val_ids_mini.npy', val_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids_mini.npy')\n",
    "val_clas =np.load(CLAS_PATH/'tmp'/'trn_ids_mini.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn_labels.shape: (1500, 1)\n",
      "squeezed trn_labels.shape: (1500,)\n"
     ]
    }
   ],
   "source": [
    "trn_labels = np.load(CLAS_PATH/'tmp'/'trn_labels_mini.npy')\n",
    "print(f'trn_labels.shape: {trn_labels.shape}')\n",
    "\n",
    "#Remove single-dimensional entries from the shape of an array.\n",
    "trn_labels = np.squeeze(trn_labels)\n",
    "print(f'squeezed trn_labels.shape: {trn_labels.shape}')\n",
    "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels_mini.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt =70\n",
    "em_sz = 400\n",
    "#number of hidden activation per LSTM layer\n",
    "n_hid= 1150\n",
    "#number of LSTM layers to use in the architecture\n",
    "n_layers = 3\n",
    "vocab_size = len(itos)\n",
    "#as big as can before run out of mem\n",
    "bs = 48\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lbl = trn_labels.min()\n",
    "trn_labels -= min_lbl\n",
    "val_labels -= min_lbl\n",
    "n_class = int(trn_labels.max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "val_ds = TextDataset(val_clas, val_labels)\n",
    "#extends torch Sampler, sort data by lenth (ish), kind of on whole shorter but a bit random\n",
    "#makes processing a little more efficient as more similar data size in batches\n",
    "trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "#note sure is was but in lesson .pynb of if library changed, added bs here\n",
    "val_samp = SortishSampler(val_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "model_data = ModelData(PATH, trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 1\n",
    "#dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dps = np.array([0.4, 0.5, 0.05, 0.3, 0.4])*0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "get_rnn_classifer() returns SequentialRNN(MultiBatchRNN(RNN_Encoder), PoolingLinearClassifier(layers, drops))\n",
    "\n",
    "where RNN_Encoder modelled after **Merity et al (2017)**\n",
    "\n",
    "\"In this work, we investigate a set of regularization strategies that are not only highly effective butwhich can also be used with no modification to existing LSTM implementations. The weight-dropped LSTM applies recurrent regulariza- tion through a DropConnectmask on the hidden-to-hidden recurrent weights. Other strategies include the use of randomized-length backpropagation through time (BPTT), embedding dropout, activation regularization (AR), and temporal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq = 20*70\n",
    "layers=[em_sz*3, 50, n_class]\n",
    "drops=[dps[4], 0.1]\n",
    "\n",
    "#get_rnn_classifer(bptt, max_seq, n_class, n_tok, emb_sz, n_hid, n_layers, pad_token, layers, drops, bidir=False,\n",
    "#     dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5):\n",
    "m = get_rnn_classifer(bptt, max_seq, n_class=n_class, n_tok=vocab_size, emb_sz=em_sz, n_hid=n_hid, \n",
    "                      n_layers=n_layers, pad_token=1, layers=layers, drops=drops, dropouti=dps[0], \n",
    "                      wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why are we changing the optimizer?\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning using cross entropy criterion\n",
    "learn = RNN_Learner(model_data, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "#seq2seq_reg  regularization, if alphs then mult by a squared fn, beta is temporal activation regularization (slowness)\n",
    "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learn.clip=25.0\n",
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "lrm = 2.6\n",
    "#discriminative weigth decay\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying out weight decay\n",
    "#wd = 1e-7\n",
    "#wd = 0\n",
    "#learn.load_encoder('lm2_enc_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "IndexError",
<<<<<<< Updated upstream
     "evalue": "index 49 is out of bounds for axis 0 with size 0",
=======
     "evalue": "index 173 is out of bounds for axis 0 with size 0",
>>>>>>> Stashed changes
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-9b045eb09223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/learner.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(self, start_lr, end_lr, wds, linear)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR_Finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcycle_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         return fit(model, data, n_epoch, layer_opt.opt, self.crit,\n\u001b[0;32m--> 198\u001b[0;31m             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, **kwargs)\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, epochs, opt, crit, metrics, callbacks, stepper, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstepper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdebias_loss\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/model.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(stepper, dl, metrics)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mbatch_cnts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_cnts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;31m# avoid py3.6 issue where queue is infinite and can result in memory exhaustion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/dataloader.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/text.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
<<<<<<< Updated upstream
      "\u001b[0;31mIndexError\u001b[0m: index 49 is out of bounds for axis 0 with size 0"
=======
      "\u001b[0;31mIndexError\u001b[0m: index 173 is out of bounds for axis 0 with size 0"
>>>>>>> Stashed changes
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "learn.lr_find(lrs/1000)\n",
    "learn.sched.plot()\n",
    "#TODO debug this error - same code runs in original lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_0_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('clas_0_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.feeze_to(-2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_1_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('clas_1_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_2_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
