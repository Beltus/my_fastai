{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Language Model\n",
    "\n",
    "Key papers:\n",
    "\n",
    "'Learned in Translation: Contextualized Word Vectors' McCann et al. 2017\n",
    "\n",
    "'Regularizing and Optimizing LSTM Language Models', Merity et al. (2017)\n",
    "\n",
    "'A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay', Smith (2018)\n",
    "\n",
    "Here we are testing on subset of imdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import html\n",
    "import sys\n",
    "from subprocess import call\n",
    "\n",
    "import torch\n",
    "\n",
    "from fastai.text import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active CUDA Device: GPU 0\n"
     ]
    }
   ],
   "source": [
    "print('Active CUDA Device: GPU', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'    #beginning of sentence tag, useful for model to know this\n",
    "FLD = 'xfld'    #data field tag\n",
    "\n",
    "PATH=Path('..')/'data/imdb/aclImdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'imdbEr.txt', 'models', 'imdb.vocab', 'README', 'train', 'tmp']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier\n",
    "CLAS_PATH=Path('..')/'data/imdb/imdb_clas'\n",
    "CLAS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "#Language Model\n",
    "LM_PATH=Path('..')/'data/imdb/imdb_lm'\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsup for unlabelled\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "\n",
    "def get_texts(path):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(CLASSES):\n",
    "        #The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell\n",
    "        for fname in (path/label).glob('*.*'):\n",
    "            #eg ../data/imdb/aclImdb/train/neg/1696_1.txt\n",
    "            texts.append(fname.open('r').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "trn_texts, trn_labels = get_texts(PATH/'train')\n",
    "val_texts, val_labels = get_texts(PATH/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(trn_texts): 75000, len(val_texts): 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'len(trn_texts): {len(trn_texts)}, len(val_texts): {len(val_texts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clip data - identifying clipped data with '_mini' postfix\n",
    "trn_texts = trn_texts[:1500]\n",
    "trn_labels = trn_labels[:1500]\n",
    "val_texts = val_texts[:500]\n",
    "val_labels = val_labels[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['labels', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make randomness reproducible\n",
    "np.random.seed(42)\n",
    "#randomly shuffle this list\n",
    "trn_idx = np.random.permutation(len(trn_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our randomly sorted training and validation lists-generally a good idea to do this\n",
    "trn_texts = trn_texts[trn_idx]\n",
    "val_texts = val_texts[val_idx]\n",
    "\n",
    "trn_labels = trn_labels[trn_idx]\n",
    "val_labels = val_labels[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':val_labels}, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for training data, remove unsupervised\n",
    "df_trn[df_trn['labels']!=2].to_csv(CLAS_PATH/'train_mini.csv',header=False, index=False)\n",
    "df_val.to_csv(CLAS_PATH/'test_mini.csv', header=False, index=False)\n",
    "\n",
    "#write the classes to a file ie neg pos unsup\n",
    "(CLAS_PATH/'classes_mini.txt').open('w').writelines(f'{o}/n' for o in CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use more data for training than the given split\n",
    "trn_texts,val_texts = sklearn.model_selection.train_test_split(np.concatenate([trn_texts, val_texts]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 200)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise classifications to zero\n",
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':[0]*len(trn_texts)}, columns=col_names)\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':[0]*len(val_texts)}, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn.to_csv(LM_PATH/'train_mini.csv', header=False, index=False)\n",
    "df_val.to_csv(LM_PATH/'test_mini.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Tokens\n",
    "\n",
    "Turn text into a a list of tokens using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this makes pandas more efficient-when passed in to pandas, returns an iterator to iterate through chunks, then loop through these chinks of the dataframe\n",
    "CHUNKSIZE = 24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile a regular expression pattern, returning a pattern object\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "#this may not catch all badly formatted text, may need to add to/modify for other input datasets\n",
    "def fixup(text_str):\n",
    "    text_str = text_str.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(text_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max thread count: 16\n"
     ]
    }
   ],
   "source": [
    "print(f'max thread count: {len(os.sched_getaffinity(0))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df, n_lbls=1):\n",
    "    #.iloc[<row_selection>,<col_selction>] here default is column 0 only\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 '+ df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls+1, len(df.columns)):\n",
    "        texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = texts.apply(fixup).values.astype(str)\n",
    "    #type(texts): <class 'pandas.core.series.Series'>\n",
    "    #uses ProcessPoolExcutor with 1/2 of the cpu's, pass in a series to tokenize\n",
    "    start = timer()\n",
    "    #significant speed up gained through multi processing\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    end = timer()\n",
    "    print(f'elapsed: {end - start}')\n",
    "    return tok, list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(tf_reader, n_lbls):\n",
    "    #iterate over the TextFileReader object in chunks\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(tf_reader):\n",
    "        print(i)\n",
    "        tok_, labels_ = get_texts(r, n_lbls)\n",
    "        tok += tok_\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(df_trn): <class 'pandas.io.parsers.TextFileReader'>\n"
     ]
    }
   ],
   "source": [
    "df_trn = pd.read_csv(LM_PATH/'train_mini.csv', header=None, chunksize=CHUNKSIZE)\n",
    "df_val = pd.read_csv(LM_PATH/'test_mini.csv', header=None, chunksize=CHUNKSIZE)\n",
    "#note is not a dataframe\n",
    "print(f'type(df_trn): {type(df_trn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "elapsed: 3.569215379997331\n",
      "0\n",
      "elapsed: 3.07083859800332\n"
     ]
    }
   ],
   "source": [
    "tok_trn, trn_labels = get_all(df_trn, 1)\n",
    "tok_val, val_labels = get_all(df_val, 1)\n",
    "#note smaller output than full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LM_PATH/'tmp').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n-xbos-xfld-1-first-thing-i-noticed-in-this-movie-of-course-,-was-the-unnecessary-amount-of-nudity-.-it-'s-not-oozing-nudity-or-anything-,-but-a-lot-that-was-not-needed-.-annik-borel-plays-a-disturbed-woman-believing-her-families-ghost-stories-that-her-ancestor-who-eerily-resembles-her-was-a-werewolf-,-and-believes-their-fate-are-destined-to-be-the-same-.-which-actually-i-found-quite-interesting-.-the-original-wolf-man-was-intended-to-be-a-completely-psychological-movie-,-but-universal-threw-in-the-actual-wolf-man-you-were-never-supposed-to-see-for-n-extra-buck-or-two-.-i-find-this-concept-of-someone-not-really-being-a-werewolf-interesting-.-unfortunately-this-is-not-the-film-i-was-searching-for-.-\\n\\n-instead-we-know-she-s-not-a-werewolf-from-the-beginning-,-so-there-'s-no-thrill-or-twist-,-also-they-attempt-to-make-the-film-seem-like-a-this-really-happened-scenario-.-they-fail-there-too-adding-one-or-two-parts-of-the-film-referring-to-this-being-reality-.-at-first-i-was-excited-upon-reading-the-description-of-the-film-.-but-i-slowly-realized-it-was-a-cover-just-so-they-could-expose-the-main-characters-breasts-as-often-as-possible-.-\\n\\n-annik-borel-is-either-a-decent-actor-playing-a-great-psychotic-role-,-or-a-really-bad-actor-playing-a-psychotic-role-.-since-the-character-danniele-has-no-brains-and-is-just-a-nut-who-runs-around-insane-and-snarling-and-snapping-like-a-wolf-,-it-takes-little-skill-to-play-.-she-has-moments-were-her-performance-breaks-through-for-a-creepy-moment-but-is-quickly-ruined-by-the-poor-camera-work-and-light-.-the-idea-is-great-,-but-hideously-executed-throughout-the-film-.-3-/-10\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first 100 chars example of data - t_up - indicates token is uppercase\n",
    "'-'.join(tok_trn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'tok_trn_mini.npy', tok_trn)\n",
    "np.save(LM_PATH/'tmp'/'tok_val_mini.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(LM_PATH/'tmp'/'tok_trn_mini.npy')\n",
    "tok_val = np.load(LM_PATH/'tmp'/'tok_val_mini.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 23957),\n",
       " ('.', 20425),\n",
       " (',', 19109),\n",
       " ('a', 11727),\n",
       " ('and', 10781),\n",
       " ('to', 10056),\n",
       " ('of', 9994),\n",
       " ('is', 7567),\n",
       " ('it', 7040),\n",
       " ('i', 6753),\n",
       " ('in', 6472),\n",
       " ('this', 5762),\n",
       " ('that', 5416),\n",
       " ('\"', 5168),\n",
       " (\"'s\", 4288),\n",
       " ('was', 4010),\n",
       " ('-', 3806),\n",
       " ('\\n\\n', 3776),\n",
       " ('movie', 3450),\n",
       " ('for', 3142),\n",
       " ('but', 3093),\n",
       " ('as', 3061),\n",
       " ('with', 3060),\n",
       " (\"n't\", 2829),\n",
       " ('film', 2800)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit as over this code gets 'clunky'\n",
    "#also for classification using >60k doesnt help anyway\n",
    "MAX_VOCAB = 60000\n",
    "MIN_FREQ = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of words, create index for this list\n",
    "\n",
    "itos: index to string\n",
    "stoi: string to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index those tokens that appear more than 2x\n",
    "itos = [o for o,c in freq.most_common(MAX_VOCAB) if c>MIN_FREQ]\n",
    "itos.insert(0, '_pad_')\n",
    "#use if not in vocab\n",
    "itos.insert(0, '_unk_')\n",
    "itos[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9574"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#default to 0 if not in dict\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index each token for each review. Call for every word, for every sentence\n",
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "trn_lm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'trn_ids_mini.npy', trn_lm)\n",
    "np.save(LM_PATH/'tmp'/'val_ids_mini.npy', val_lm)\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos_mini.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_lm = np.load(LM_PATH/'tmp'/'trn_ids_mini.npy')\n",
    "val_lm = np.load(LM_PATH/'tmp'/'val_ids_mini.npy')\n",
    "itos = pickle.load(open(LM_PATH/'tmp'/'itos_mini.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9574, 1800)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(itos)\n",
    "VOCAB_SIZE, len(trn_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wikitext103 Conversion\n",
    "\n",
    "42:00\n",
    "\n",
    "Instead of pretraining on Imagenet, for NLP we can pretrain on a large subset of Wikipedia\n",
    "\n",
    "If pre-train classifier by first creating a language model then fine tune that as a classifier-helpful in L4 2017\n",
    "\n",
    "IMDB not that different to english docs - train a good @ English LM then fine tune.\n",
    "\n",
    "S. Merity created WikiText-103 contains all articles extracted from Wikipedia (ignoring smaller atricles):\n",
    "\n",
    "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/\n",
    "\n",
    "JH trained this Language Model - start with these weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -nH -r -np http://files.fast.ai/models/wt103/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our model needs to have exactly the same embedding size, number of hidden layers and number of layers as per Jeremy's wikitext103 LM\n",
    "EMBEDDING_SIZE = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = PATH/'models'/'wt103'\n",
    "PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.load uses Python's unpickling facilities but treats storages, which underlie tensors, specially\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eg row_m: array([-0.0183 , -0.13826,  0.01438, -0.01285,  0.00407,  0.01944,  0.01149, -0.13282, -0.02295, ... ], dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to map our itos to itos for wikitext, which is easy as we have the itos for wikitext103\n",
    "itos_wiki = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
    "#-1 means not in wikitext dictionary\n",
    "stoi_wiki = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos_wiki)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty embedding matrix\n",
    "new_wgts = np.zeros((VOCAB_SIZE, EMBEDDING_SIZE), dtype=np.float32)\n",
    "#go through every work in imdb vocab\n",
    "for i, w in enumerate(itos):\n",
    "    #look it up in wiki vocab\n",
    "    r = stoi_wiki[w]\n",
    "    #use mean if our string doesnt exist in wiki (-1 means not in wikitext dict)\n",
    "    new_wgts[i] = enc_wgts[r] if r>=0 else row_m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T: convert to torch tensor and put on gpu. Replace our weights\n",
    "wgts['0.encoder.weight'] = T(new_wgts)\n",
    "wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_wgts))\n",
    "#decoder (turns final prediction back into a word) uses same weights\n",
    "wgts['1.decoder.weight'] = T(np.copy(new_wgts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "Word2Vec is a single embedding matrix - each word has a vector and thats it.\n",
    "\n",
    "A single layer (input layer) form a pre-trained linear model - pre-trained on a co-ocurrence matrix. No reason to beleive it has learnt anything about the English languguage nor do we expect it to have any great capabilities.\n",
    "\n",
    "This language model had a 400 dimensional embedding matrix, 3 hidden layers with 1150 activations per layer + reg - state of the art AWD LSTM.\n",
    "\n",
    "Single layer of linear model vs 3 layer RNN - very different capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of hidden activation per LSTM layer\n",
    "n_hid= 1150\n",
    "#number of LSTM layers to use in the architecture\n",
    "n_layers = 3\n",
    "wd=1e-7\n",
    "#grab 70 at a time\n",
    "bptt=70\n",
    "bs=52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "#for nlp better to use the defaults below\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(504184, 9695)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate all our docs together\n",
    "t = len(np.concatenate(trn_lm))\n",
    "t, t//bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For our Language Model we concatenate all docs, will be continually trying to predict: what is the next word\n",
    "#(The 3 lines below are the same code Jeremy also used to train the wikitext103 model from scratch)\n",
    "#We start by initializing the LanguageModelLoader with a big list of numbers (all docs concatenated together).\n",
    "#Then we batchify this - break total data into bs pieces ie here 64 pieces.\n",
    "#ie data = data.reshape(self.bs, -1).T \n",
    "#will thus have 64 columns and total_dala/64 rows\n",
    "        \n",
    "#batchify and get_batch very similar to awd-lstm-lm.utils functions\n",
    "        \n",
    "#*1:05 - key ideas here, review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when we iterate over LanguageModelLoader, the sequence length is changed with a normal distribution and significantly changed 5% of time\n",
    "#on first iteration: seq_len = bptt+5*5 ie 95 here\n",
    "#subseqently: seq_len = max(5, int(np.random.normal(bptt, 5))) where 5% of time the mean bptt=bptt/2\n",
    "#see awd-lstm-lm.finetune.train()\n",
    "trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "\n",
    "#LanguageModelData returns a RNN_Learner with SequentialRNN model\n",
    "#(The SequentialRNN layer is the native torch's Sequential wrapper that puts the RNN_Encoder and LinearDecoder layers sequentially in the model.)\n",
    "model_data = LanguageModelData(PATH, pad_idx=1, nt=VOCAB_SIZE, trn_dl=trn_dl, val_dl=val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropouts - through experimentation these work well.\n",
    "#less data need more dropout - good ratios, just tune the multiplier (if overfitting increase the multiplier)\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a model data object we gan grab the model - which will give us a learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout: dropout to apply to the activations going from one LSTM layer to another\n",
    "#dropouti (float): dropout to apply to the input layer.\n",
    "#wdrop (float): dropout used for a LSTM's internal (or hidden) recurrent weights.\n",
    "#dropoute (float): dropout to apply to the embedding layer.\n",
    "#dropouth (float): dropout to apply to the activations going from one LSTM layer to another\n",
    "kwargs = {'dropouti': drops[0], 'dropout': drops[1], 'wdrop': drops[2], 'dropoute': drops[3], 'dropouth': drops[4]}\n",
    "\n",
    "#returns a RNN_Learner with model ~ SequentialRNN(RNN_Encoder(...), LinearDecoder(...))\n",
    "learner = model_data.get_model(opt_fn = opt_fn, emb_sz = EMBEDDING_SIZE, n_hid = n_hid, n_layers = n_layers, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "all the work happens in get_model(). - the key part - where we implement **AWD LSTM** and use the backbone+head\n",
    "\n",
    "Creates an encoder (RNN_Encoder) then creates a SequentialRNN and sticks on top of the encoder a LinearDecoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Learner.unfreeze of SequentialRNN(\n",
       "  (0): RNN_Encoder(\n",
       "    (encoder): Embedding(9574, 400, padding_idx=1)\n",
       "    (encoder_with_dropout): EmbeddingDropout(\n",
       "      (embed): Embedding(9574, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDrop(\n",
       "        (module): LSTM(400, 1150, dropout=0.105)\n",
       "      )\n",
       "      (1): WeightDrop(\n",
       "        (module): LSTM(1150, 1150, dropout=0.105)\n",
       "      )\n",
       "      (2): WeightDrop(\n",
       "        (module): LSTM(1150, 400, dropout=0.105)\n",
       "      )\n",
       "    )\n",
       "    (dropouti): LockedDropout(\n",
       "    )\n",
       "    (dropouths): ModuleList(\n",
       "      (0): LockedDropout(\n",
       "      )\n",
       "      (1): LockedDropout(\n",
       "      )\n",
       "      (2): LockedDropout(\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=9574, bias=False)\n",
       "    (dropout): LockedDropout(\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.metrics = [accuracy]\n",
    "learner.unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual we do a fit just on the last layer. The way this is setup the last layer is the embedding weights-which will be the thing that is most wrong.\n",
    "\n",
    "Train a single epoch of just the embedding weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "lrs=lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cdb27e840b4ca593891017a85a08f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      4.638049   4.279665   0.249158  \n",
      "\n",
      "elapsed: 15.54901929100015\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)\n",
    "end = timer()\n",
    "print(f'elapsed: {end - start}')\n",
    "#showing seq_len and bptt values:\n",
    "# 0%|          | 0/6870 [00:00<?, ?it/s]initial seq_len: 95, bptt: 70\n",
    "# 0%|          | 1/6870 [00:01<2:29:15,  1.30s/it, loss=11]seq_len: 36, bptt: 70\n",
    "# 0%|          | 2/6870 [00:01<1:20:31,  1.42it/s, loss=11]seq_len: 73, bptt: 70\n",
    "# 0%|          | 3/6870 [00:01<1:00:29,  1.89it/s, loss=11]seq_len: 73, bptt: 70\n",
    "# 0%|          | 4/6870 [00:01<50:36,  2.26it/s, loss=11]  seq_len: 65, bptt: 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_loss is a cross entropy loss (perplexity = e^val_loss), accuracy is how often do we get next word correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm_last_fit_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.load('lm_last_fit_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7618544fcdbd461eac3e0486059247fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      4.568792   4.334835   0.235171  \n",
      "\n",
      "elapsed: 14.154311100995983\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)\n",
    "end = timer()\n",
    "print(f'elapsed: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start doing a few epochs of the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f502b279554481ba3c1f182465f734e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      4.470828   4.169173   0.256953  \n",
      "    1      4.283084   4.078315   0.264669  \n",
      "    2      4.072157   4.056928   0.265615  \n",
      "    3      3.917307   4.040404   0.267662  \n",
      "    4      3.757429   4.067      0.265158  \n",
      "    5      3.657999   4.071597   0.265406  \n",
      "    6      3.530312   4.101998   0.265179  \n",
      "    7      3.392758   4.154758   0.26106   \n",
      "    8      3.284303   4.19387    0.258834  \n",
      "    9      3.190743   4.220111   0.258699  \n",
      "    10     3.144208   4.224221   0.260195  \n",
      "    11     3.062013   4.264408   0.256011  \n",
      "    12     3.104647   4.24776    0.258343  \n",
      "    13     2.966249   4.296583   0.256571  \n",
      "    14     2.976444   4.30655    0.2571    \n",
      "\n",
      "elapsed: 213.9563166070002\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)\n",
    "end = timer()\n",
    "print(f'elapsed: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without using wiki103 weights\n",
    "#epoch      trn_loss   val_loss   accuracy   \n",
    "#    0      6.364579   6.188742   0.046889  \n",
    "#    14     4.899093   4.919841   0.187322 \n",
    "#with\n",
    "#epoch      trn_loss   val_loss   accuracy   \n",
    "#    0      4.470828   4.169173   0.256953  \n",
    "#    14     2.976444   4.30655    0.2571  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.21750212099356"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(4.307)  #2 years ago perplixity state of art was >100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss compares with Lesson 4 - Academic worlds best in 2017 - aftewr 14 epochs val loss of 4.23. \n",
    "\n",
    "Here after 1 epoch on full model we have a 4.12 loss \n",
    "\n",
    "ie by pretraining on wikitext103 better loss after 1 epoch than best loss for Lesson 4 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for full dataset after 5 hrs (on GTX1080Ti):\n",
    "#14     4.045284   4.072163   0.299391  \n",
    "\n",
    "#1/50th dataset after 213 seconds (356 seconds on GTX1070):\n",
    "#14     4.899093   4.919841   0.187322  \n",
    "\n",
    "#1/100th dataset after 96 seconds:\n",
    "#14     5.937064   5.737786   0.097723  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save('lm1_mini') #saves whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_encoder('lm1_enc_mini') #saves just the encoder part of the sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv(CLAS_PATH/'train_mini.csv', header=None, chunksize=CHUNKSIZE)\n",
    "df_val = pd.read_csv(CLAS_PATH/'test_mini.csv', header=None, chunksize=CHUNKSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "elapsed: 3.3929726330025005\n"
     ]
    }
   ],
   "source": [
    "tok_trn, trn_labels = get_all(tf_reader=df_trn, n_lbls=1)\n",
    "tok_val, val_labels = get_all(tf_reader=df_trn, n_lbls=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CLAS_PATH/'tmp').mkdir(exist_ok=True)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'tok_trn_mini.npy', tok_trn)\n",
    "np.save(CLAS_PATH/'tmp'/'tok_val_mini.npy', tok_val)\n",
    "\n",
    "np.save(CLAS_PATH/'tmp'/'trn_labels_mini.npy', trn_labels)\n",
    "np.save(CLAS_PATH/'tmp'/'val_labels_mini.npy', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#about to reload same encoder, reuse our tokens from languiage model\n",
    "tok_trn = np.load(CLAS_PATH/'tmp'/'tok_trn_mini.npy')\n",
    "tok_val = np.load(CLAS_PATH/'tmp'/'tok_val_mini.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9574"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos = pickle.load((LM_PATH/'tmp'/'itos_mini.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_clas = np.array([[stoi[o] for o in p] for p in tok_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(CLAS_PATH/'tmp'/'trn_ids_mini.npy', trn_clas)\n",
    "np.save(CLAS_PATH/'tmp'/'val_ids_mini.npy', val_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_clas = np.load(CLAS_PATH/'tmp'/'trn_ids_mini.npy')\n",
    "val_clas =np.load(CLAS_PATH/'tmp'/'trn_ids_mini.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn_labels.shape: (1500, 1)\n",
      "squeezed trn_labels.shape: (1500,)\n"
     ]
    }
   ],
   "source": [
    "trn_labels = np.load(CLAS_PATH/'tmp'/'trn_labels_mini.npy')\n",
    "print(f'trn_labels.shape: {trn_labels.shape}')\n",
    "\n",
    "#Remove single-dimensional entries from the shape of an array.\n",
    "trn_labels = np.squeeze(trn_labels)\n",
    "print(f'squeezed trn_labels.shape: {trn_labels.shape}')\n",
    "val_labels = np.squeeze(np.load(CLAS_PATH/'tmp'/'val_labels_mini.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt =70\n",
    "em_sz = 400\n",
    "#number of hidden activation per LSTM layer\n",
    "n_hid= 1150\n",
    "#number of LSTM layers to use in the architecture\n",
    "n_layers = 3\n",
    "vocab_size = len(itos)\n",
    "#as big as can before run out of mem\n",
    "bs = 48\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_lbl = trn_labels.min()\n",
    "trn_labels -= min_lbl\n",
    "val_labels -= min_lbl\n",
    "n_class = int(trn_labels.max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "val_ds = TextDataset(val_clas, val_labels)\n",
    "#extends torch Sampler, sort data by lenth (ish), kind of on whole shorter but a bit random\n",
    "#makes processing a little more efficient as more similar data size in batches\n",
    "trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "#note sure is was but in lesson .pynb of if library changed, added bs here\n",
    "val_samp = SortishSampler(val_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "model_data = ModelData(PATH, trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 1\n",
    "dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dps = np.array([0.4, 0.5, 0.05, 0.3, 0.4])*0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "get_rnn_classifer() returns SequentialRNN(MultiBatchRNN(RNN_Encoder), PoolingLinearClassifier(layers, drops))\n",
    "\n",
    "where RNN_Encoder modelled after **Merity et al (2017)**\n",
    "\n",
    "\"In this work, we investigate a set of regularization strategies that are not only highly effective butwhich can also be used with no modification to existing LSTM implementations. The weight-dropped LSTM applies recurrent regulariza- tion through a DropConnectmask on the hidden-to-hidden recurrent weights. Other strategies include the use of randomized-length backpropagation through time (BPTT), embedding dropout, activation regularization (AR), and temporal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq = 20*70\n",
    "layers=[em_sz*3, 50, n_class]\n",
    "drops=[dps[4], 0.1]\n",
    "\n",
    "#get_rnn_classifer(bptt, max_seq, n_class, n_tok, emb_sz, n_hid, n_layers, pad_token, layers, drops, bidir=False,\n",
    "#     dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5):\n",
    "m = get_rnn_classifer(bptt, max_seq, n_class=n_class, n_tok=vocab_size, emb_sz=em_sz, n_hid=n_hid, \n",
    "                      n_layers=n_layers, pad_token=1, layers=layers, drops=drops, dropouti=dps[0], \n",
    "                      wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why are we changing the optimizer?\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning using cross entropy criterion\n",
    "learn = RNN_Learner(model_data, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "#seq2seq_reg  regularization, if alphs then mult by a squared fn, beta is temporal activation regularization (slowness)\n",
    "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learn.clip=25.0\n",
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "lrm = 2.6\n",
    "#discriminative weigth decay\n",
    "#lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying out weight decay\n",
    "#wd = 1e-7\n",
    "#wd = 0\n",
    "#learn.load_encoder('lm2_enc_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 173 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-9b045eb09223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/learner.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(self, start_lr, end_lr, wds, linear)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR_Finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrn_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_geom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcycle_len\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcycle_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle_mult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         return fit(model, data, n_epoch, layer_opt.opt, self.crit,\n\u001b[0;32m--> 198\u001b[0;31m             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, **kwargs)\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, epochs, opt, crit, metrics, callbacks, stepper, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstepper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdebias_loss\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/model.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(stepper, dl, metrics)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mbatch_cnts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mbatch_cnts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;31m# avoid py3.6 issue where queue is infinite and can result in memory exhaustion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/dataloader.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/963GB/Data/Python/Courses/fastai/my_fastai/dl2/fastai/text.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 173 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "learn.lr_find(lrs/1000)\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_0_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('clas_0_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.feeze_to(-2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_1_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('clas_1_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_2_mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
